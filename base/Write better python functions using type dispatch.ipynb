{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "august-caution",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Write-Better-Python-Functions-(Using-Type-Dispatch)\" data-toc-modified-id=\"Write-Better-Python-Functions-(Using-Type-Dispatch)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Write Better Python Functions (Using Type Dispatch)</a></span></li><li><span><a href=\"#Getting-Started\" data-toc-modified-id=\"Getting-Started-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Getting Started</a></span></li><li><span><a href=\"#Use-Case\" data-toc-modified-id=\"Use-Case-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Use Case</a></span><ul class=\"toc-item\"><li><span><a href=\"#Briefing\" data-toc-modified-id=\"Briefing-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Briefing</a></span></li><li><span><a href=\"#How-do-we-go-about-this?\" data-toc-modified-id=\"How-do-we-go-about-this?-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>How do we go about this?</a></span></li><li><span><a href=\"#Hiccup\" data-toc-modified-id=\"Hiccup-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Hiccup</a></span></li></ul></li><li><span><a href=\"#Solution\" data-toc-modified-id=\"Solution-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Solution</a></span><ul class=\"toc-item\"><li><span><a href=\"#Iteration-1\" data-toc-modified-id=\"Iteration-1-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Iteration 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Objective:\" data-toc-modified-id=\"Objective:-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Objective:</a></span></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Code</a></span></li><li><span><a href=\"#What-have-we-done?\" data-toc-modified-id=\"What-have-we-done?-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>What have we done?</a></span></li><li><span><a href=\"#Quick-Check\" data-toc-modified-id=\"Quick-Check-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Quick Check</a></span></li><li><span><a href=\"#What-can-we-improve?\" data-toc-modified-id=\"What-can-we-improve?-4.1.5\"><span class=\"toc-item-num\">4.1.5&nbsp;&nbsp;</span>What can we improve?</a></span></li></ul></li><li><span><a href=\"#Iteration-2\" data-toc-modified-id=\"Iteration-2-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Iteration 2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Objective</a></span></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Code</a></span></li><li><span><a href=\"#What-have-we-done?\" data-toc-modified-id=\"What-have-we-done?-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>What have we done?</a></span></li><li><span><a href=\"#Quick-Check\" data-toc-modified-id=\"Quick-Check-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Quick Check</a></span></li><li><span><a href=\"#What-can-we-improve?\" data-toc-modified-id=\"What-can-we-improve?-4.2.5\"><span class=\"toc-item-num\">4.2.5&nbsp;&nbsp;</span>What can we improve?</a></span></li></ul></li><li><span><a href=\"#Iteration-3\" data-toc-modified-id=\"Iteration-3-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Iteration 3</a></span><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Objective</a></span></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Code</a></span></li><li><span><a href=\"#What-have-we-done?\" data-toc-modified-id=\"What-have-we-done?-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>What have we done?</a></span></li><li><span><a href=\"#Quick-Check\" data-toc-modified-id=\"Quick-Check-4.3.4\"><span class=\"toc-item-num\">4.3.4&nbsp;&nbsp;</span>Quick Check</a></span></li><li><span><a href=\"#What-can-we-improve?\" data-toc-modified-id=\"What-can-we-improve?-4.3.5\"><span class=\"toc-item-num\">4.3.5&nbsp;&nbsp;</span>What can we improve?</a></span></li></ul></li><li><span><a href=\"#Iteration-4\" data-toc-modified-id=\"Iteration-4-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Iteration 4</a></span><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Objective</a></span></li><li><span><a href=\"#Type-Dispatch\" data-toc-modified-id=\"Type-Dispatch-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Type Dispatch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-for-Type-Dispatch\" data-toc-modified-id=\"Example-for-Type-Dispatch-4.4.2.1\"><span class=\"toc-item-num\">4.4.2.1&nbsp;&nbsp;</span>Example for Type Dispatch</a></span></li></ul></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-4.4.3\"><span class=\"toc-item-num\">4.4.3&nbsp;&nbsp;</span>Code</a></span></li><li><span><a href=\"#What-have-we-done?\" data-toc-modified-id=\"What-have-we-done?-4.4.4\"><span class=\"toc-item-num\">4.4.4&nbsp;&nbsp;</span>What have we done?</a></span></li><li><span><a href=\"#Quick-Check\" data-toc-modified-id=\"Quick-Check-4.4.5\"><span class=\"toc-item-num\">4.4.5&nbsp;&nbsp;</span>Quick Check</a></span></li><li><span><a href=\"#What-can-we-improve?\" data-toc-modified-id=\"What-can-we-improve?-4.4.6\"><span class=\"toc-item-num\">4.4.6&nbsp;&nbsp;</span>What can we improve?</a></span></li></ul></li></ul></li><li><span><a href=\"#Closing-Thoughts\" data-toc-modified-id=\"Closing-Thoughts-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Closing Thoughts</a></span></li><li><span><a href=\"#Credits\" data-toc-modified-id=\"Credits-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Credits</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-material",
   "metadata": {},
   "source": [
    "# Write Better Python Functions (Using Type Dispatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-numbers",
   "metadata": {},
   "source": [
    "Writing a python function is easy but writing a good function not so much. Most python programmers are often not aware of how to write clear and concise functions, let alone using Type Dispatch. Let us see how we can write better function in python using Type Dispatch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-gardening",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-olive",
   "metadata": {},
   "source": [
    "Before we get into what is Type Dispatch? or How Type dispatch can make you a better python programmer, let us get some basics out of the way like *What is a good python function?* To be honest, I would rather let [Jeff Knupp](https://medium.com/hackernoon/write-better-python-functions-c3a9a36382a6) explain it. \n",
    "\n",
    "I truly believe that only way to really understand how something works, is by breaking it into granular pieces and putting it back together or building it from scratch. Let us take a very well defined use case and write a function for it from scratch. By scratch, I mean like starting with one to one translation of the pseudo code into python code without thinking about any best practices. Then by *analyzing what works & what didn't*, we can build on top of this. We are going to repeat this process until we run out of ideas to improve the code. So this post is going to be not one, but many versions of a function for the same use case.\n",
    "\n",
    " *If you are wondering Why?* Because we are going to unlearning everything we know about writing a function and question each line of code as we write it. By this way, we can clearly see how each piece fits. Rather than blindly following a check list of items about writing a function, we will create our own check list. In case this post missed something that is of value, then please let me know I will gladly update it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-jason",
   "metadata": {},
   "source": [
    "<img src=\"Python_Function_Cover.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-ocean",
   "metadata": {},
   "source": [
    "We will try to write the best or at least my best python function for a particular [use case](#Use-Case) in 4 iterations. Each iteration is going to be an improvement over the last one, with clear objective declared upfront. Getting better at anything is an iterative process, depending on where you are in the spectrum of python proficiency, one of the 4 iterations is going to resonate more with you than others. Next step of this post is  understanding the use case. Buckle up, it's going to be a very interesting journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-lafayette",
   "metadata": {},
   "source": [
    "# Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-grove",
   "metadata": {},
   "source": [
    "## Briefing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-dressing",
   "metadata": {},
   "source": [
    "Convert the given image input into pytorch tensor. Yes, that's it. Quite simple right?\n",
    "\n",
    "Folks with data science background can skip this part and jump to [this](#Hiccup) section, as I am going to give a brief introduction about python libraries needed for this use case.\n",
    "\n",
    "1. [**Pillow**](https://pillow.readthedocs.io/en/stable/) is a <u>*Python Imaging Library*</u> that adds support for opening, manipulating, and saving many different image file formats. \n",
    "2. [**Numpy**](https://numpy.org) stands for Numerical Python. Numpy is the core library for scientific computing in Python. It provides a high-performance <u>*multidimensional array*</u> object, and tools for working with these arrays.\n",
    "3. [**PyTorch**](https://pytorch.org) is an open source <u>*Machine Learning Library*</u>, it provides two major functions namely <u>*Tensor*</u> computing (like NumPy) with strong acceleration via <u>*GPU*</u> & Deep neural networks built on a type-based automatic differentiation system. Pytorch can run the code using CPU if there is no CUDA supported GPU available but this will be way slower for training a ML model.\n",
    "\n",
    "If you wondering why or to whom this use case is useful? Most Machine Learning applications (Face detection, OCR) with image data as input are using these libraries. This use case is considered data preprocessing in ML world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-visit",
   "metadata": {},
   "source": [
    "## How do we go about this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-characterization",
   "metadata": {},
   "source": [
    "The steps involved are very straight forward. Open the image using Pillow library, and convert it to numpy array & transpose it before converting to Tensor. BTW, Gentle reminder - This post is about writing better python functions and not a data pre-processing tutorial for deep learning so if it's intimidating, don't worry about it. You will feel more comfortable, once we start coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-leonard",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T17:40:41.610277Z",
     "start_time": "2021-04-01T17:40:41.485958Z"
    }
   },
   "source": [
    "![Use Case - Process Steps](Images/Use%20Case%20-%20Process%20Steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-driving",
   "metadata": {},
   "source": [
    "Couple of points to take note about the process steps before we on.\n",
    "\n",
    "1. PIL image when converted to Numpy array, it takes shape - (Height, Width, # of Channels). In Pytorch the image tensor must be of the shape - (# of Channels, Height, Width) so we going to `transpose` the array before converting it to tensor.\n",
    "\n",
    "\n",
    "2. For experienced Pytorch users who are wondering, why the hell are we not using torch functions like `ToTensor` transform method and skip Numpy? Yep, you are right, we can absolutely do that but to drive the point of this post home better, I have consciously decided to take the longer route."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-content",
   "metadata": {},
   "source": [
    "## Hiccup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-platform",
   "metadata": {},
   "source": [
    "Sorry, the use case is not that simple. There is a small hiccup, The function should support multiple input data types, namely `str/Path`, `Pillow(Image)` and `Numpy(Ndarray)`. \n",
    "\n",
    "The process step defined in the above image only supports the data types `str/Path` as input. Therefore we need to add support for `Pillow(Image)` and `Numpy(Ndarray)` types, but if we think about it, these data types are intermediate steps in converting the image file to torch tensor. So there is really no additional step from the above image, we just have to duplicate the steps defined for `str/Path` data types and remove few initial steps to support `Pillow(Image)` and `Numpy(Ndarray)` data types as input for our function.\n",
    "\n",
    "Process steps for 2 additional Workflows are as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-shadow",
   "metadata": {},
   "source": [
    "<center> Image Input </center> | <center> Array Input </center> \n",
    "- | - \n",
    "![Image Input](Images/Use%20Case%20-%20Sub%20Process%20Steps%20(Image)_960_shorttitle.png)  | ![Array Input](Images/Use%20Case%20-%20Sub%20Process%20Steps%20(Array)_960_shorttitle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-history",
   "metadata": {},
   "source": [
    "After comparing 3 images it is very clear that:\n",
    "\n",
    "- Image data type's process steps are subset of the original process steps.\n",
    "- Array data type's process steps are subset of the Image data type process steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-blake",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-native",
   "metadata": {},
   "source": [
    "First 2 iterations of the function is going to be about the **behavior implementation** whereas last two iterations are all about code **refactoring**. If you are one of those readers who likes to read the ending of a book first (I am not judging), Click [here](#Iteration-4) to jump straight to the final version. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-sewing",
   "metadata": {},
   "source": [
    "## Iteration 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-grade",
   "metadata": {},
   "source": [
    "### Objective:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-raising",
   "metadata": {},
   "source": [
    "Make it work. As simple as that, initially when we are starting from scratch, it is best just to focus on getting the basic feature or functionality work. We don't need all the bells and whistles. For this iteration we will focus on only implementing [Use Process Steps](#How-do-we-go-about-this?) and not worry about [Hiccup.](#Hiccup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-chest",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "intelligent-outreach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:07:59.822505Z",
     "start_time": "2021-04-04T15:07:59.818098Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "# Set Torch device to GPU whether CUDA supported GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Function to convert image to Torch tensor\n",
    "def Tensor(inp):\n",
    "    \n",
    "    # Read image from disk using PIL Image\n",
    "    img = PILImage.open(inp).convert('RGB')\n",
    "    \n",
    "    # Convert the image to numpy ndarray\n",
    "    imgArr = np.asarray(img, np.uint8)\n",
    "    \n",
    "    # Rearrange the shape of the array so that it is pytorch compatible\n",
    "    imgArr = imgArr.transpose(2, 0, 1)\n",
    "    \n",
    "    # Convert Numpy array to Torch Tensor and move it to device\n",
    "    return torch.from_numpy(imgArr).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-spiritual",
   "metadata": {},
   "source": [
    "### What have we done?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-venice",
   "metadata": {},
   "source": [
    "We managed to translate the [process steps](#How-do-we-go-about-this?) into code. The number of lines in code almost matches the number of boxes in the diagram. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-gasoline",
   "metadata": {},
   "source": [
    "### Quick Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-glance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T10:13:38.279847Z",
     "start_time": "2021-04-02T10:13:38.275591Z"
    }
   },
   "source": [
    "We need a sample image to test our function so we will download one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "arctic-cache",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:06:49.067793Z",
     "start_time": "2021-04-04T15:06:48.785850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sample.png', <http.client.HTTPMessage at 0x7fcbfbf19050>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download a sample image to disk\n",
    "import urllib.request \n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/200px-Python-logo-notext.svg.png\"\n",
    "filename = \"sample.png\"\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-chest",
   "metadata": {},
   "source": [
    "Now, let us take our function for a test drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rocky-glance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:08:03.274824Z",
     "start_time": "2021-04-04T15:08:03.270488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pass the sample image through our function\n",
    "imgTensor = Tensor(filename)\n",
    "\n",
    "# Check whether the output of the function is a Tensor\n",
    "assert isinstance(imgTensor, torch.Tensor), \"Not a Tensor\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-vault",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T11:29:41.681072Z",
     "start_time": "2021-04-02T11:29:41.670501Z"
    }
   },
   "source": [
    "Looks like we managed to nail the basic functionality. Awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-gibson",
   "metadata": {},
   "source": [
    "### What can we improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-filing",
   "metadata": {},
   "source": [
    "* We are yet to implement the multiple data type input functionality i.e support for data types like `PIL Image`, `Ndarray`. \n",
    "* Also we should definitely improve name of the function, it doesn't really say anything about the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-morning",
   "metadata": {},
   "source": [
    "## Iteration 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-occurrence",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-steel",
   "metadata": {},
   "source": [
    "When we add support for more than one data type, we need to be very careful because without proper validation, things can go south real quick and from end user point of view the error messages won't make any sense. So in this iteration we will:\n",
    "\n",
    "1. Implement multiple data type support and validations for these types. \n",
    "2. Make the function name more useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-butler",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aboriginal-specific",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:08:36.195486Z",
     "start_time": "2021-04-04T15:08:36.187838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image as PILImage\n",
    "from pathlib import Path, PurePath\n",
    "\n",
    "# Set Torch device to GPU whether CUDA supported GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Function to convert image to Torch tensor\n",
    "def from_multiInput_toTensor(inp):\n",
    "    \n",
    "    # Input is of type str or Path, then read from disk & convert to array\n",
    "    if isinstance(inp, (str, PurePath)):\n",
    "        try: \n",
    "            image = PILImage.open(inp).convert('RGB')\n",
    "            imageArray = np.asarray(image.copy(), np.uint8)\n",
    "        except Exception as error:\n",
    "            raise error\n",
    "            \n",
    "    # Input is of type PIL Image, then we convert it to array      \n",
    "    elif isinstance(inp, PILImage.Image):\n",
    "        imageArray = np.asarray(inp, np.uint8)\n",
    "        \n",
    "    # Input is of type ndarray, then assign it to imageArray variable  \n",
    "    elif isinstance(inp, np.ndarray):\n",
    "        imageArray = inp\n",
    "        \n",
    "    # Raise TypeError with input type is not supported\n",
    "    else: \n",
    "        raise TypeError(\"Input must be of Type - String or Path or PIL Image or Numpy array\")\n",
    "        \n",
    "    # Rearrange the shape of the array so that it is pytorch compatible\n",
    "    if imageArray.ndim == 3: imageArray = imageArray.transpose(2, 0, 1)\n",
    "        \n",
    "    # Convert Numpy array to Torch Tensor and move it to device\n",
    "    return torch.from_numpy(imageArray).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-clerk",
   "metadata": {},
   "source": [
    "### What have we done?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-messaging",
   "metadata": {},
   "source": [
    "We have managed to implement all the functionalities required for this use case. Since `ndarray` is the last data type before the output data type - `tensor`, we convert the input first to `ndarray` for every supported data type and transpose it at the end, right before converting it to `tensor`. By adopting this style of coding, we are able to avoid having to convert the input to `tensor` & `return` the value for every supported data type. Instead we do it only once at the end.\n",
    "\n",
    "\n",
    "With the help of `isinstance` function, we are able to identify the data type of the input and notify the users by raising `TypeError` with appropriate error message if any unsupported data type is passed. Often with io operations, something can go wrong easily so if the data type is `str` or `Path`, we read the image file inside `try` & `except` block and let the user know about the error (if any).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-genius",
   "metadata": {},
   "source": [
    "### Quick Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-charleston",
   "metadata": {},
   "source": [
    "Before we proceed, we will write a helper function to check whether two tensors are same or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "veterinary-constitutional",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:06:34.354019Z",
     "start_time": "2021-04-04T15:06:34.350838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test if two torch tensors are same or not\n",
    "def is_same_tensor(tensor1, tensor2):\n",
    "    assert torch.eq(tensor1, tensor2).all(), \"The Tensors are not equal!\"\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-campaign",
   "metadata": {},
   "source": [
    "When writing a test function, better to throw a proper error and not simple print function which can get buried in between other messages. We will also check a couple of things and ensure that the updated version is all good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-gothic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:34:18.484638Z",
     "start_time": "2021-04-02T13:34:18.481070Z"
    }
   },
   "source": [
    "1. Is the output of the functions from last two iterations the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "controversial-pension",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:08:43.978807Z",
     "start_time": "2021-04-04T15:08:43.971083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the output of two versions are same or not\n",
    "is_same_tensor(Tensor(filename), from_multiInput_toTensor(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-shame",
   "metadata": {},
   "source": [
    "2. Is the support for multiple data types working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bibliographic-conservation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:08:48.296367Z",
     "start_time": "2021-04-04T15:08:48.282649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the support for Path\n",
    "path = Path(Path.cwd(), filename).resolve()\n",
    "is_same_tensor(Tensor(filename), from_multiInput_toTensor(path))\n",
    "\n",
    "# Check the support for PIL Image\n",
    "image = PILImage.open(filename).convert('RGB')\n",
    "is_same_tensor(Tensor(filename), from_multiInput_toTensor(image))\n",
    "\n",
    "# Check the support for Ndarray\n",
    "imageArray = np.asarray(image, np.uint8)\n",
    "is_same_tensor(Tensor(filename), from_multiInput_toTensor(imageArray))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-appointment",
   "metadata": {},
   "source": [
    "3. Are the validations working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bored-trouble",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:09:00.879266Z",
     "start_time": "2021-04-04T15:09:00.855759Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d7f024b1580d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Validate whether an error is thrown when user passes wrong file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfrom_multiInput_toTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-02314fc6379d>\u001b[0m in \u001b[0;36mfrom_multiInput_toTensor\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mimageArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Input is of type PIL Image, then we convert it to array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-02314fc6379d>\u001b[0m in \u001b[0;36mfrom_multiInput_toTensor\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPurePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPILImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mimageArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tidbits/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2891\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2892\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.png'"
     ]
    }
   ],
   "source": [
    "# Validate whether an error is thrown when user passes wrong file\n",
    "from_multiInput_toTensor('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fiscal-scroll",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:09:04.627025Z",
     "start_time": "2021-04-04T15:09:04.615034Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input must be of Type - String or Path or PIL Image or Numpy array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9287df34893f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Validate whether an error is thrown when user passes wrong file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfrom_multiInput_toTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-02314fc6379d>\u001b[0m in \u001b[0;36mfrom_multiInput_toTensor\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Raise TypeError with input type is not supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input must be of Type - String or Path or PIL Image or Numpy array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Rearrange the shape of the array so that it is pytorch compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input must be of Type - String or Path or PIL Image or Numpy array"
     ]
    }
   ],
   "source": [
    "# Validate whether an error is thrown when input type is list\n",
    "from_multiInput_toTensor([filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-logan",
   "metadata": {},
   "source": [
    "Great! We didn't break anything. Now let us move on with refractoring as we are done with behavior implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-realtor",
   "metadata": {},
   "source": [
    "### What can we improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-treaty",
   "metadata": {},
   "source": [
    "* With the goal of adding support for multiple data types, we sort of wrote a Spaghetti code. The life cycle of the variable `imageArray` is confusing at the least, this type of code design is not very intuitive and can make life hell for the person who is going to maintain this code base. Of course, for our simple use case this is not case but in the spirit of writing better function, let us better not to get used to this type of coding style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-irish",
   "metadata": {},
   "source": [
    "* The function name is a lot better but still not ideal. Why? Because it still does not clearly state what the input type for the function is. Playing trail & error with `TypeError` is definitely not the write way of coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-gossip",
   "metadata": {},
   "source": [
    "* As the name of the function suggest it takes many inputs which means we got lot of moving parts in one function. Long run this may lead to many unintended side effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-tissue",
   "metadata": {},
   "source": [
    "## Iteration 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-tomato",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-begin",
   "metadata": {},
   "source": [
    "After taking a closer look at the points from last section, it is very clear that we need to break the function into 3 smaller ones. i.e one function for each data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-norway",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "herbal-coast",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:13:13.981759Z",
     "start_time": "2021-04-04T15:13:13.973417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image as PILImage\n",
    "from pathlib import Path, PurePath\n",
    "\n",
    "# Change numpy array to torch tensor\n",
    "def numpy_ToImageTensor(imageArray):\n",
    "    \n",
    "    # Check if the input is really ndarray and if not then raise error\n",
    "    if not isinstance(imageArray, np.ndarray):\n",
    "        raise TypeError(\"Input must be of Type - Numpy array\")\n",
    "    \n",
    "    # Set Torch device to GPU whether CUDA supported GPU is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Transpose the numpy array before converting it to Tensor\n",
    "    if imageArray.ndim == 3: imageArray = imageArray.transpose(2, 0, 1)\n",
    "    return torch.from_numpy(imageArray).to(device)\n",
    "\n",
    "# Change image to torch tensor\n",
    "def pil_ToImageTensor(image):\n",
    "    \n",
    "    # Check if the input is really PIL Image and if not then raise error\n",
    "    if not isinstance(image, PILImage.Image):\n",
    "        raise TypeError(\"Input must be of Type - PIL image\")\n",
    "        \n",
    "    # Convert the image to numpy \n",
    "    imageArray = np.array(image)\n",
    "    \n",
    "    # Call the numpy_ToImageTensor function before returning the value\n",
    "    return numpy_ToImageTensor(imageArray)\n",
    "\n",
    "# Change image file to torch tensor\n",
    "def file_ToImageTensor(file):\n",
    "    \n",
    "    # Check if the input is string or Path and if not then raise error\n",
    "    if not isinstance(file, (str, PurePath)):\n",
    "        raise TypeError(\"Input must be of Type - String or Path\")\n",
    "        \n",
    "    # Read the image from disk and raise error (if any)\n",
    "    try: \n",
    "        image = PILImage.open(file).convert('RGB')\n",
    "    except Exception as error:\n",
    "        raise error\n",
    "    \n",
    "    # Call the pil_ToImageTensor function before returning the value\n",
    "    return pil_ToImageTensor(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-progress",
   "metadata": {},
   "source": [
    "### What have we done?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-ethnic",
   "metadata": {},
   "source": [
    "This is good progress, we removed the spaghetti code and added modularity. Now it is like we have linear chain of functions where two of them are calling another one, which makes it easier to understand and maintain.\n",
    "\n",
    "`file_ToImageTensor` -> `pil_ToImageTensor` -> `numpy_ToImageTensor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-crest",
   "metadata": {},
   "source": [
    "Modularity makes adding new feature quite straight forward and easy to test as we have to alter only one specific function. Some examples for functions to modify for hypothetical feature request:\n",
    "1. `file_ToImageTensor` - Ability to read black & white image as well.\n",
    "\n",
    "2. `pil_ToImageTensor`  - Resize the image before converting it to Tensor.\n",
    "\n",
    "Please do note that even though you didn't touch the rest of code base for the above stated examples, it is best to test other functions as well to avoid any surprise. \n",
    "\n",
    "Modular design paved the way for better function names and parameter names as the input type have become more specific. \n",
    "\n",
    "| | From | | To |\n",
    "|:-- |:--|---|:--|\n",
    "|Function Name: |from_multiInput_toTensor | |file_ToImageTensor<br/> pil_ToImageTensor <br/>  numpy_ToImageTensor|\n",
    "|Parameter Name: |inp | |file <br/>  image <br/>  imageArray|\n",
    "\n",
    "These subtle changes can a go long way in making the code more user friendly. Honestly at this point, I feel like I am preaching, so I will try to wrap this quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-clothing",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/ffbinQwuCWrrW/giphy.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-mauritius",
   "metadata": {},
   "source": [
    "### Quick Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-retailer",
   "metadata": {},
   "source": [
    "This post is getting too long for own liking, so I am not going to test negative scenarios or different data types again and only check for the original use case. You really shouldn't skip testing after refractoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "stylish-optimization",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:30:36.326302Z",
     "start_time": "2021-04-04T15:30:36.317959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_same_tensor(from_multiInput_toTensor(filename), file_ToImageTensor(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-graham",
   "metadata": {},
   "source": [
    "### What can we improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-basketball",
   "metadata": {},
   "source": [
    "* Looks like we follow the same pattern of checking data type with `isinstance` function and raising `TypeError` for every function, which is a kind of code repetition as well. \n",
    "\n",
    "* Okay, we have to do something about the function names. I am pretty sure no one will remember these names, definitely not my future self."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-collectible",
   "metadata": {},
   "source": [
    "## Iteration 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-burton",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-stable",
   "metadata": {},
   "source": [
    "Improve the documentation and avoid code repetition with the help of Type Dispatch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-worcester",
   "metadata": {},
   "source": [
    "### Type Dispatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-parts",
   "metadata": {},
   "source": [
    "Type dispatch allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia & Swift. All we have to do is add a decorator `typedispatch` before our function. Probably, it is easier to demonstrate than to explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-cooper",
   "metadata": {},
   "source": [
    "#### Example for Type Dispatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-emergency",
   "metadata": {},
   "source": [
    "Function definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "brave-record",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:32:20.632238Z",
     "start_time": "2021-04-04T15:32:20.585113Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastcore.all import *\n",
    "from typing import List\n",
    "\n",
    "# Function to multiply two ndarrays\n",
    "@typedispatch\n",
    "def multiple(x:np.ndarray, y:np.ndarray ): \n",
    "    return x * y\n",
    "\n",
    "# Function to multiply a List by an integer\n",
    "@typedispatch\n",
    "def multiple(lst:List, x:int): \n",
    "    return [ x*val for val in lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-blackjack",
   "metadata": {},
   "source": [
    "Calling 1st function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "upset-leeds",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:32:23.361571Z",
     "start_time": "2021-04-04T15:32:23.357594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is [1 2]\n",
      "y is 10\n",
      "Result of multiplying two numpy arrays: [10 20]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(1,3)\n",
    "print(f'x is {x}')\n",
    "\n",
    "y = np.array(10)\n",
    "print(f'y is {y}')\n",
    "\n",
    "print(f'Result of multiplying two numpy arrays: { multiple(x, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-first",
   "metadata": {},
   "source": [
    "Calling 2nd function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "smooth-creature",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:32:27.480640Z",
     "start_time": "2021-04-04T15:32:27.476575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is [1, 2]\n",
      "y is 10\n",
      "Result of multiplying a List of integers by an integer: [10, 20]\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2]\n",
    "print(f'x is {x}')\n",
    "\n",
    "y = 10\n",
    "print(f'y is {y}')\n",
    "\n",
    "print(f'Result of multiplying a List of integers by an integer: {multiple(x, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-screw",
   "metadata": {},
   "source": [
    "<p> Life of a programmer can be made so much better if they don't have to <u> come up with different function names </u> with no change in purpose (for various data types). If this doesn't encourage you to use Type Dispatch whenever possible then I don't know what will &#129335;</p>\n",
    "\n",
    "We will be using [fastcore](https://fastcore.fast.ai) package for the implementing [Type Dispatch](https://fastcore.fast.ai/dispatch.html#TypeDispatch) to our use case. For more details on fastcore and Type Dispatch, check this awesome [blog](https://fastpages.fast.ai/fastcore/) by [Hamel Husain](https://twitter.com/hamelhusain). Also check out [fastai](https://docs.fast.ai), which inspired me to write this post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-natural",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "partial-vault",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:38:04.642315Z",
     "start_time": "2021-04-04T15:38:04.633444Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image as PILImage\n",
    "from pathlib import Path, PurePath\n",
    "from fastcore.all import *\n",
    "\n",
    "\n",
    "@typedispatch\n",
    "def to_imageTensor(arr: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Change ndarray to torch tensor.\n",
    "    \n",
    "    The ndarray would be of the shape (Height, Width, # of Channels)\n",
    "    but pytorch tensor expects the shape as \n",
    "    (# of Channels, Height, Width) before putting \n",
    "    the Tensor on GPU if it's available.\n",
    "    \n",
    "    Args:\n",
    "        arr[ndarray]: Ndarray which needs to be \n",
    "        converted to torch tensor\n",
    "    \n",
    "    Returns:\n",
    "        Torch tensor on GPU (if it's available)   \n",
    "    \"\"\"\n",
    "    \n",
    "    # Set Torch device to GPU whether CUDA supported GPU is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Transpose the array before converting to tensor\n",
    "    imgArr = arr.transpose(2, 0, 1) if arr.ndim == 3 else arr\n",
    "    return torch.Tensor(imgArr).to(device)\n",
    "\n",
    "\n",
    "@typedispatch\n",
    "def to_imageTensor(image: PILImage.Image) -> torch.Tensor:\n",
    "    \"\"\"Change image to torch tensor.\n",
    "    \n",
    "    The PIL image cast as numpy array with dtype as uint8,\n",
    "    and then passed to to_imageTensor(arr: np.ndarray) function\n",
    "    for converting numpy array to torch tensor.\n",
    "    \n",
    "    Args:\n",
    "        image[PILImage.Image]: PIL Image which \n",
    "        needs to be converted to torch tensor\n",
    "    \n",
    "    Returns:\n",
    "        Torch tensor on GPU (if it's available)   \n",
    "    \n",
    "    \"\"\"\n",
    "    return to_imageTensor(np.asarray(image, np.uint8))\n",
    "\n",
    "\n",
    "@typedispatch\n",
    "def to_imageTensor(file: (str, PurePath)) -> torch.Tensor:\n",
    "    \"\"\"Change image file to torch tensor.\n",
    "    \n",
    "    Read the image from disk as 3 channels (RGB) using PIL, \n",
    "    and passed on to to_imageTensor(image: PILImage.Image)\n",
    "    function for converting Image to torch tensor.\n",
    "    \n",
    "    Args:\n",
    "        file[str, PurePath]: Image file name which needs to\n",
    "        be converted to torch tensor\n",
    "    \n",
    "    Returns:\n",
    "        Torch tensor on GPU (if it's available)\n",
    "    \n",
    "    Raises:\n",
    "        Any error thrown while reading the image file,\n",
    "        Mostly FileNotFoundError will be raised.\n",
    "    \n",
    "    \"\"\"\n",
    "    try: \n",
    "        img = PILImage.open(file).convert('RGB')\n",
    "    except Exception as error:\n",
    "        raise error\n",
    "    return to_imageTensor(img)\n",
    "\n",
    "\n",
    "@typedispatch\n",
    "def to_imageTensor(x:object) -> None:\n",
    "    \"\"\"For unsupported data types, raise TypeError. \"\"\"\n",
    "    raise TypeError('Input must be of Type - String or Path or PIL Image or Numpy array')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-marathon",
   "metadata": {},
   "source": [
    "### What have we done?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-broadcast",
   "metadata": {},
   "source": [
    "By utilizing the Type dispatch functionality, we managed to use the same name for all 3 functions, and each one's behavior is differentiated by their input type. The function name is also shortened with the removal of input type. This makes the function name easier to remember."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-massage",
   "metadata": {},
   "source": [
    "By calling the function name, we can see what are the different input types supported by the function. Fastcore by default expects two input parameters, since we have only it assigns second one as object. The second object parameter will not have any impact on the function behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "current-leather",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T13:38:59.343728Z",
     "start_time": "2021-04-04T13:38:59.339651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ndarray,object) -> to_imageTensor\n",
       "(Image,object) -> to_imageTensor\n",
       "(str,object) -> to_imageTensor\n",
       "(PurePath,object) -> to_imageTensor\n",
       "(object,object) -> to_imageTensor"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_imageTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-drilling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T13:34:24.794559Z",
     "start_time": "2021-04-04T13:34:24.790836Z"
    }
   },
   "source": [
    "With help of inspect module, we can access the input & output types of a particular function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "capable-argentina",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T13:43:50.103364Z",
     "start_time": "2021-04-04T13:43:50.099140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (arr: numpy.ndarray) -> torch.Tensor>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.signature(to_imageTensor[np.ndarray])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-prototype",
   "metadata": {},
   "source": [
    "The docstrings implemented in this iteration makes the code more readable. The docstring of a particular input type can be accessed by calling __doc__ along with the input type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "herbal-detector",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T13:42:39.544710Z",
     "start_time": "2021-04-04T13:42:39.541256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change ndarray to torch tensor.\n",
      "    \n",
      "    The ndarray would be of the shape (Height, Width, # of Channels)\n",
      "    but pytorch tensor expects the shape as \n",
      "    (# of Channels, Height, Width) before putting \n",
      "    the Tensor on GPU if it's available.\n",
      "    \n",
      "    Args:\n",
      "        arr[ndarray]: Ndarray which needs to be \n",
      "        converted to torch tensor\n",
      "    \n",
      "    Returns:\n",
      "        Torch tensor on GPU (if it's available)   \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(to_imageTensor[np.ndarray].__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-shame",
   "metadata": {},
   "source": [
    "As discussed in the last section, we managed to move the `TypeError` message to a separate function with input type as `object`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-conviction",
   "metadata": {},
   "source": [
    "### Quick Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-switzerland",
   "metadata": {},
   "source": [
    "Still working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "collective-requirement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:41:27.477689Z",
     "start_time": "2021-04-04T15:41:27.470436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_same_tensor(file_ToImageTensor(filename), to_imageTensor(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-anderson",
   "metadata": {},
   "source": [
    "Validating error message when unsupported data type is passed to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "plastic-uncle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T15:41:29.558453Z",
     "start_time": "2021-04-04T15:41:29.531740Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input must be of Type - String or Path or PIL Image or Numpy array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-670871404220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mto_imageTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tidbits/lib/python3.7/site-packages/fastcore/dispatch.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minst\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-762a887ec3cf>\u001b[0m in \u001b[0;36mto_imageTensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_imageTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;34m\"\"\"For unsupported data types, raise TypeError. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input must be of Type - String or Path or PIL Image or Numpy array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Input must be of Type - String or Path or PIL Image or Numpy array"
     ]
    }
   ],
   "source": [
    "to_imageTensor([filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-development",
   "metadata": {},
   "source": [
    "### What can we improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-collins",
   "metadata": {},
   "source": [
    "This is it. I believe that we are done here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-tucson",
   "metadata": {},
   "source": [
    "# Closing Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-crowd",
   "metadata": {},
   "source": [
    "Did we really write the best function for this [use case](#Use-Case)? I think so, but don't take my word. If you think you have a better version or literally any other version, reach out to me via [Twitter](https://twitter.com/@6aravind) or the comments section. I will gladly update this post based on your comments.\n",
    "\n",
    "Even though Type Dispatch was the focal point when I started writing this post, soon I realized that the code evolution process is equally important too. So I have decided to include that as well. I hope you enjoyed this journey of writing a python function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-mortgage",
   "metadata": {},
   "source": [
    "# Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-wyoming",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T13:55:29.049733Z",
     "start_time": "2021-04-04T13:55:29.046428Z"
    }
   },
   "source": [
    "[Giphy](https://giphy.com)\n",
    "\n",
    "Icons:\n",
    "   1.  <a target=\"_blank\" href=\"https://www.iconbros.com/icons/ib-b-video-card\">Video Card </a> icon by <a target=\"_blank\" href=\"https://iconbros.com\">IconBros</a>\n",
    "   2. [zwicon](https://www.zwicon.com/)\n",
    "   \n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "290.8571472167969px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
